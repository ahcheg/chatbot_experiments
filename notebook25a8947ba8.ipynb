{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\nclass TransformerBlock(nn.Module):\n\n    def __init__(self, input_size, is_layer_norm=False):\n        super(TransformerBlock, self).__init__()\n        self.is_layer_norm = is_layer_norm\n        if is_layer_norm:\n            self.layer_morm = nn.LayerNorm(normalized_shape=input_size)\n\n        self.relu = nn.ReLU()\n        self.linear1 = nn.Linear(input_size, input_size)\n        self.linear2 = nn.Linear(input_size, input_size)\n        self.init_weights()\n\n    def init_weights(self):\n        init.xavier_normal_(self.linear1.weight)\n        init.xavier_normal_(self.linear2.weight)\n\n    def FFN(self, X):\n        return self.linear2(self.relu(self.linear1(X)))\n\n    def forward(self, Q, K, V, episilon=1e-8):\n        '''\n        :param Q: (batch_size, max_r_words, embedding_dim)\n        :param K: (batch_size, max_u_words, embedding_dim)\n        :param V: (batch_size, max_u_words, embedding_dim)\n        :return: output: (batch_size, max_r_words, embedding_dim)  same size as Q\n        '''\n        dk = torch.Tensor([max(1.0, Q.size(-1))]).cuda()\n\n        Q_K = Q.bmm(K.permute(0, 2, 1)) / (torch.sqrt(dk) + episilon)\n        Q_K_score = F.softmax(Q_K, dim=-1)  # (batch_size, max_r_words, max_u_words)\n        V_att = Q_K_score.bmm(V)\n\n        if self.is_layer_norm:\n            X = self.layer_morm(Q + V_att)  # (batch_size, max_r_words, embedding_dim)\n            output = self.layer_morm(self.FFN(X) + X)\n        else:\n            X = Q + V_att\n            output = self.FFN(X) + X\n\n        return output\n\ndef dot_attention(q, k, v, v_mask=None, dropout=None):\n    attention_weights = torch.matmul(q, k.transpose(-1, -2))\n    if v_mask is not None:\n        attention_weights *= v_mask.unsqueeze(1)\n    attention_weights = F.softmax(attention_weights, -1)\n    if dropout is not None:\n        attention_weights = dropout(attention_weights)\n    output = torch.matmul(attention_weights, v)\n    return output\n\n\nclass Attention(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Attention, self).__init__()\n        self.linear1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n        self.linear2 = nn.Linear(in_features=hidden_size, out_features=1)\n        self.init_weights()\n\n    def init_weights(self):\n        init.xavier_normal_(self.linear1.weight)\n        init.xavier_normal_(self.linear2.weight)\n\n    def forward(self, X, mask=None):\n        '''\n        :param X:\n        :param mask:   http://juditacs.github.io/2018/12/27/masked-attention.html\n        :return:\n        '''\n        M = F.tanh(self.linear1(X))  # (batch_size, max_u_words, embedding_dim)\n        M = self.linear2(M)\n        if mask is not None:\n            M[~mask] = float('-inf')\n        score = F.softmax(M, dim=1)    # (batch_size, max_u_words, 1)\n\n        output = (score * X).sum(dim=1)  # (batch_size, embedding_dim)\n        return output\n\n\n\nclass IMPChat(nn.Module):\n    '''\n        A pytorch version of Sequential Matching Network which is proposed in\n            \"Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots\"\n    '''\n    def __init__(self, word_embeddings, args): #TODO\n        self.args = args\n        super(IMPChat, self).__init__()\n        self.word_embedding = nn.Embedding(num_embeddings=len(word_embeddings), embedding_dim=args.emb_len, padding_idx=0,\n                                           _weight=torch.FloatTensor(word_embeddings))\n        \n        self.alpha =  nn.Parameter(torch.tensor(0.5))\n        self.n_layer = args.n_layer\n        self.max_hop = args.max_hop\n\n        self.selector_transformer = TransformerBlock(input_size=args.emb_len)\n        self.W_word = nn.Parameter(data=torch.Tensor(args.emb_len, args.emb_len, args.max_utterances))\n        self.v = nn.Parameter(data=torch.Tensor(args.max_utterances, 1))\n\n        self.linear_word = nn.Linear(2*args.max_words, 1)\n        self.linear_score = nn.Linear(in_features=self.max_hop, out_features=1)\n\n        self.transformer_r = []\n        for i in range(self.n_layer):\n            self.transformer_r.append(TransformerBlock(input_size=args.emb_len))\n        self.transformer_r = nn.ModuleList(self.transformer_r)\n\n        self.transformer_rp = []\n        for i in range(self.n_layer+1):\n            self.transformer_rp.append(TransformerBlock(input_size=args.emb_len))\n        self.transformer_rp = nn.ModuleList(self.transformer_rp)\n\n        self.transformer_utt = TransformerBlock(input_size=args.emb_len)\n        self.transformer_res = TransformerBlock(input_size=args.emb_len)\n        self.transformer_ur = TransformerBlock(input_size=args.emb_len)\n        self.transformer_ru = TransformerBlock(input_size=args.emb_len)\n\n        self.A1 = nn.Parameter(data=torch.Tensor(args.emb_len, args.emb_len))\n        self.A2 = nn.Parameter(data=torch.Tensor(args.emb_len, args.emb_len))\n        self.A3 = nn.Parameter(data=torch.Tensor(args.emb_len, args.emb_len))\n\n        self.cnn_2d_1 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(3,3))\n        self.maxpooling1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n\n        self.cnn_2d_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3))\n        self.maxpooling2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n\n        self.cnn_2d_3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3))\n        self.maxpooling3 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n\n\n        self.cnn_2d_4 = nn.Conv2d(in_channels=(self.n_layer+1)*2, out_channels=16, kernel_size=(3,3))\n\n        self.cnn_2d_5 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3))\n\n        self.cnn_2d_6 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3))\n\n\n        self.affine2 = nn.Linear(in_features=3*3*64, out_features=args.emb_len)\n        self.affine3 = nn.Linear(in_features=3*3*64, out_features=args.emb_len)\n\n        self.gru_acc = nn.GRU(input_size=args.emb_len, hidden_size=args.gru_hidden, batch_first=True)\n        self.attention = Attention(input_size=args.emb_len, hidden_size=args.gru_hidden)\n\n        self.affine_out = nn.Linear(in_features=args.gru_hidden + args.emb_len, out_features=1)\n\n        self.tanh = nn.Tanh()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.init_weights()\n        print(self)\n\n\n    def init_weights(self):\n        init.uniform_(self.W_word)\n        init.uniform_(self.v)\n        init.uniform_(self.linear_word.weight)\n        init.uniform_(self.linear_score.weight)\n\n        init.xavier_normal_(self.A1)\n        init.xavier_normal_(self.A2)\n        init.xavier_normal_(self.A3)\n        init.xavier_normal_(self.cnn_2d_1.weight)\n        init.xavier_normal_(self.cnn_2d_2.weight)\n        init.xavier_normal_(self.cnn_2d_3.weight)\n        init.xavier_normal_(self.affine2.weight)\n        init.xavier_normal_(self.affine3.weight)\n        init.xavier_normal_(self.affine_out.weight)\n        for weights in [self.gru_acc.weight_hh_l0, self.gru_acc.weight_ih_l0]:\n            init.orthogonal_(weights)\n\n\n    def word_selector(self, key, context):\n        '''\n        :param key:  (bsz, max_u_words, d)\n        :param context:  (bsz, max_u_words, d)\n        :return: score:\n        '''\n        dk = torch.sqrt(torch.Tensor([self.args.emb_len])).cuda()\n        A = torch.tanh(torch.einsum(\"blrd,ddh,bud->blruh\", context, self.W_word, key)/dk)\n        A = torch.einsum(\"blruh,hp->blrup\", A, self.v).squeeze()   # b x l x u x u\n\n        a = torch.cat([A.max(dim=2)[0], A.max(dim=3)[0]], dim=-1) # b x l x 2u\n        s1 = torch.softmax(self.linear_word(a).squeeze(), dim=-1)  # b x l\n        return s1\n\n    def utterance_selector(self, key, context):\n        '''\n        :param key:  (bsz, max_u_words, d)\n        :param context:  (bsz, max_u_words, d)\n        :return: score:\n        '''\n        key = key.mean(dim=1)\n        context = context.mean(dim=2)\n        s2 = torch.einsum(\"bud,bd->bu\", context, key)/(1e-6 + torch.norm(context, dim=-1)*torch.norm(key, dim=-1, keepdim=True) )\n        return s2\n\n    def distance(self, A, B, C, epsilon=1e-6):\n        M1 = torch.einsum(\"bud,dd,brd->bur\", [A, B, C]) / torch.sqrt(torch.tensor(200.0))\n\n        A_norm = A.norm(dim=-1)\n        C_norm = C.norm(dim=-1)\n        M2 = torch.einsum(\"bud,brd->bur\", [A, C]) / (torch.einsum(\"bu,br->bur\", A_norm, C_norm) + epsilon)\n        return M1, M2\n\n    def context_selector(self, context, max_hop=3):\n        '''\n        :param context: (batch_size, max_utterances, max_u_words, embedding_dim)\n        :param key: (batch_size, max_u_words, embedding_dim)\n        :return:\n        '''\n        posts = context[:,0::2,:,:]\n        su1, su2, su3, su4 = posts.size()\n        context_ = posts.reshape(-1, su3, su4)   # (batch_size*max_utterances, max_u_words, embedding_dim)\n        context_ = self.selector_transformer(context_, context_, context_)\n        context_ = context_.view(su1, su2, su3, su4)\n        \n        multi_match_score = []\n        key_list = []\n        index_cache = []\n        for hop_i in range(1, max_hop+1):\n            if hop_i == 1:\n                key = posts[:, -1:, :, :]\n                key_list.append(key)\n                key = key.mean(dim=1)\n                for i in range(su1):\n                    index_cache.append([su2 - hop_i])\n                max_indice = [su2 - hop_i] * su1\n            else:\n                batch_keys = []\n                for i in range(su1):\n                    max_index = max_indice[i]\n                    batch_keys.append(posts[i, max_index,:,:])\n                key_list.append(torch.stack(batch_keys, dim=0).unsqueeze(1))\n                key = torch.cat(key_list, dim=1).mean(dim=1) \n\n            key = self.selector_transformer(key, key, key)\n            s1 = self.word_selector(key, context_)\n            s2 = self.utterance_selector(key, context_)\n            s = self.alpha * s1 + (1 - self.alpha) * s2\n            topk = s.topk(hop_i+1, dim=1)[1].tolist()\n            for i in range(su1):\n                for k in topk[i]:\n                    if k not in index_cache[i]:\n                        max_indice[i] = k\n                        index_cache[i].append(k)\n                        break\n\n            s = s.unsqueeze(-1).repeat(1,1,2).view(su1,-1)[:,:-1]\n            multi_match_score.append(s)\n        multi_match_score = torch.stack(multi_match_score, dim=-1)\n        match_score = self.linear_score(multi_match_score).squeeze()\n        context = context * match_score.unsqueeze(dim=-1).unsqueeze(dim=-1)\n\n        return context\n\n    def get_Matching_Map(self, bU_embedding, bR_embedding):\n        '''\n        :param bU_embedding: (batch_size*max_utterances, max_u_words, embedding_dim)\n        :param bR_embedding: (batch_size*max_utterances, max_r_words, embedding_dim)\n        :return: E: (bsz*max_utterances, max_u_words, max_r_words)\n        '''\n        M1, M2 = self.distance(bU_embedding, self.A1, bR_embedding)\n\n        Hu = self.transformer_utt(bU_embedding, bU_embedding, bU_embedding)\n        Hr = self.transformer_res(bR_embedding, bR_embedding, bR_embedding)\n        M3, M4 = self.distance(Hu, self.A2, Hr)\n\n        Hur = self.transformer_ur(bU_embedding, bR_embedding, bR_embedding)\n        Hru = self.transformer_ru(bR_embedding, bU_embedding, bU_embedding)\n        M5, M6 = self.distance(Hur, self.A3, Hru)\n\n        M = torch.stack([M1, M2, M3, M4, M5, M6], dim=1)  # (bsz*max_utterances, channel, max_u_words, max_r_words)\n        return M\n\n\n    def UR_Matching(self, bU_embedding, bR_embedding):\n        '''\n        :param bU_embedding: (batch_size*max_utterances, max_u_words, embedding_dim)\n        :param bR_embedding: (batch_size*max_utterances, max_r_words, embedding_dim)\n        :return: (bsz*max_utterances, (max_u_words - width)/stride + 1, (max_r_words -height)/stride + 1, channel)\n        '''\n        M = self.get_Matching_Map(bU_embedding, bR_embedding)\n\n        Z = self.relu(self.cnn_2d_1(M))\n        Z = self.maxpooling1(Z)\n\n        Z = self.relu(self.cnn_2d_2(Z))\n        Z =self.maxpooling2(Z)\n\n        Z = self.relu(self.cnn_2d_3(Z))\n        Z =self.maxpooling3(Z)\n\n        Z = Z.view(Z.size(0), -1)  # (bsz*max_utterances, *)\n\n        V = self.tanh(self.affine2(Z))   # (bsz*max_utterances, 50)\n        return V\n\n    def personalized_style_matching(self, bU_embedding, bR_embedding):\n        pre_res_stack = []\n        res_stack = []\n        pre_p_stack = []\n        q_stack = []\n\n        rp_stack = []\n        rq_stack = []\n\n        pre_res_emb = bU_embedding[:,1::2,:,:].contiguous()\n        pre_pos_emb = bU_embedding[:,0:-1:2,:,:].contiguous()\n        query_emb = bU_embedding[:,-1,:,:]\n        sr1, sr2, sr3 = bR_embedding.size()\n\n        sp1, sp2, sp3, sp4 = pre_res_emb.size()\n        pre_res_emb = pre_res_emb.view(-1, sp3, sp4)\n        pre_pos_emb = pre_pos_emb.view(-1, sp3, sp4)\n\n        res_stack.append(bR_embedding.unsqueeze(dim=1).repeat(1, sp2, 1, 1).view(-1, sr2, sr3))\n        q_stack.append(query_emb.unsqueeze(dim=1).repeat(1, sp2, 1, 1).view(-1, sr2, sr3))\n        \n        pre_res_stack.append(pre_res_emb)\n        pre_p_stack.append(pre_pos_emb)\n\n\n        for i, att in enumerate(self.transformer_r):\n            pre_res_emb = att(pre_res_emb, pre_res_emb, pre_res_emb)\n            pre_res_stack.append(pre_res_emb)\n\n            pre_pos_emb = att(pre_pos_emb, pre_pos_emb, pre_pos_emb)\n            pre_p_stack.append(pre_pos_emb)\n\n            bR_embedding = att(bR_embedding, bR_embedding, bR_embedding)\n            res_stack.append(bR_embedding.unsqueeze(dim=1).repeat(1, sp2, 1, 1).view(-1, sr2, sr3))\n\n            query_emb = att(query_emb, query_emb, query_emb)\n            q_stack.append(query_emb.unsqueeze(dim=1).repeat(1, sp2, 1, 1).view(-1, sr2, sr3))\n\n\n        for i in range(self.n_layer+1):\n            rp_stack.append(self.transformer_rp[i](pre_res_stack[i], pre_p_stack[i], pre_p_stack[i]))\n            rq_stack.append(self.transformer_rp[i](res_stack[i], q_stack[i], q_stack[i]))\n\n        res_stack.extend(rq_stack)\n        pre_res_stack.extend(rp_stack)\n\n        res_stack = torch.stack(res_stack, dim=1) # 7 x bs*14 x maxlen x emb\n        pre_res_stack = torch.stack(pre_res_stack, dim=1) # 7 x bs*14 x maxlen x emb\n\n        M = torch.einsum(\n            'bcid,bcjd->bcij',(res_stack, pre_res_stack)) / torch.sqrt(torch.tensor(200.0)) # bs*14 x 7 x maxlen x maxlen\n        \n        Z = self.relu(self.cnn_2d_4(M))\n        Z = self.maxpooling1(Z)\n\n        Z = self.relu(self.cnn_2d_5(Z))\n        Z =self.maxpooling2(Z)\n\n        Z = self.relu(self.cnn_2d_6(Z))\n        Z =self.maxpooling3(Z)\n\n        Z = Z.view(Z.size(0), -1)  # (bsz*max_utterances, *)\n\n        V = self.tanh(self.affine3(Z))   # (bsz*max_utterances, 200)\n        return V\n\n        \n    def post_aware_personalized_preference_matching(self, bU_embedding, bR_embedding):\n        multi_context = self.context_selector(bU_embedding, max_hop=self.max_hop)\n        su1, su2, su3, su4 = multi_context.size()\n        multi_context = multi_context.view(-1, su3, su4)   # (batch_size*max_utterances, max_u_words, embedding_dim)\n\n        sr1, sr2, sr3= bR_embedding.size()   # (batch_size, max_r_words, embedding_dim)\n        bR_embedding = bR_embedding.unsqueeze(dim=1).repeat(1, su2, 1, 1)  # (batch_size, max_utterances, max_r_words, embedding_dim)\n        bR_embedding = bR_embedding.view(-1, sr2, sr3)   # (batch_size*max_utterances, max_r_words, embedding_dim)\n\n        V = self.UR_Matching(multi_context, bR_embedding)\n        V = V.view(su1, su2, -1)  # (bsz, max_utterances, 300)\n        self.gru_acc.flatten_parameters()\n        H, _ = self.gru_acc(V)  # (bsz, max_utterances, rnn2_hidden)\n        return H\n        \n\n    def forward(self, bU, bR):\n        '''\n        :param bU: batch utterance, size: (batch_size, max_utterances, max_u_words)\n        :param bR: batch responses, size: (batch_size, max_r_words)\n        :return: scores, size: (batch_size, )\n        '''\n\n        bU_embedding = self.word_embedding(bU) \n        bR_embedding = self.word_embedding(bR) \n        su1, su2, su3, su4 = bU_embedding.size()\n        g_s = self.personalized_style_matching(bU_embedding, bR_embedding)\n        g_s = g_s.view(su1, su2 // 2, -1)\n        \n        g_p = self.post_aware_personalized_preference_matching(bU_embedding, bR_embedding)\n\n        L1 = self.dropout(self.attention(g_s).squeeze(-1))\n        L2 = self.dropout(g_p[:,-1,:])\n\n        context_matching = self.affine_out(torch.cat([L1,L2], dim=-1))\n        return context_matching.squeeze()\n        \n    def load_model(self, path):\n        self.load_state_dict(state_dict=torch.load(path))\n        if torch.cuda.is_available(): self.cuda()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-25T03:25:27.923941Z","iopub.execute_input":"2022-11-25T03:25:27.924387Z","iopub.status.idle":"2022-11-25T03:25:29.590357Z","shell.execute_reply.started":"2022-11-25T03:25:27.924355Z","shell.execute_reply":"2022-11-25T03:25:29.589049Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset\n\n\n\nclass DialogueDataset(TensorDataset):\n\n    def __init__(self, X_utterances, X_responses, y_labels=None):\n        super(DialogueDataset, self).__init__()\n        X_utterances = torch.LongTensor(X_utterances)\n\n        X_responses = torch.LongTensor(X_responses)\n        print(\"X_utterances: \", X_utterances.size())\n        print(\"X_responses: \", X_responses.size())\n\n        if y_labels is not None:\n            y_labels = torch.FloatTensor(y_labels)\n            print(\"y_labels: \", y_labels.size())\n            self.tensors = [X_utterances, X_responses, y_labels]\n        else:\n            self.tensors = [X_utterances, X_responses]\n\n    def __getitem__(self, index):\n        return tuple(tensor[index] for tensor in self.tensors)\n\n    def __len__(self):\n        return len(self.tensors[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.utils as utils\nimport torch.optim as optim\nimport os\nimport sys\nimport logging\nimport time\n\nfrom metrics import Metrics\nfrom tqdm import tqdm\n\nlog = logging.getLogger(\"impchat\")\n\ntorch.backends.cudnn.benchmark = True\ntorch.manual_seed(0)\ntorch.cuda.manual_seed_all(0)\n\n\ndef init_log(log, args):\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    ts = time.strftime('%Y-%m-%d_%H:%M:%S',time.localtime())\n    output_file = f'log/{ts}_{args.task}_{args.max_utterances}_{args.max_words}_{args.batch_size}_impchat.log'\n    fh = logging.FileHandler(output_file, mode='a')\n    formatter = logging.Formatter('%(asctime)-15s %(message)s')\n    fh.setFormatter(formatter)\n    log.addHandler(fh)\n\ndef get_model_obj(model: nn.Module):\n    return model.module if hasattr(model, 'module') else model\n\nclass Trainer:\n\n    def __init__(self, model, train_set, test_set, y_train, y_dev, args):\n        self.patience = 0\n        self.init_clip_max_norm = 5.0\n        self.best_result = [0, 0, 0, 0, 0, 0]\n        self.metrics = Metrics(args.score_file_path, args.type_file)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = model.to(self.device)\n        if args.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n        init_log(log, args)\n        log.info(str(model))\n        self.train_set = train_set\n        self.test_set = test_set\n        self.y_dev = y_dev\n        self.y_train = y_train\n        self.args = args\n        self.loss_func = nn.BCEWithLogitsLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.args.learning_rate, weight_decay=self.args.l2_reg)\n\n    def train_step(self, i, data):\n        with torch.no_grad():\n            batch_u, batch_r, batch_y = (item.cuda(device=self.device) for item in data)\n\n        self.optimizer.zero_grad()\n        logits = self.model(batch_u, batch_r) #, logits2\n        loss = self.loss_func(logits, target=batch_y)\n        if self.args.n_gpu > 1:\n            loss = loss.mean()\n        loss.backward()\n        self.optimizer.step()\n        print('Batch[{}] - loss: {:.6f}  batch_size:{}'.format(i, loss.item(), batch_y.size(0)) )  # , accuracy, corrects\n        return loss\n\n\n    def to_train(self):\n        log.info('Start to train...')\n        log.info(f'train set: {len(self.y_train)}')\n        log.info(f'test set: {len(self.y_dev)}')\n\n        for epoch in range(self.args.epochs):\n            print(\"\\nEpoch \", epoch+1, \"/\", self.args.epochs)\n            avg_loss = 0\n\n            self.model.train()\n            for i, data in enumerate(self.train_set):\n                loss = self.train_step(i, data)\n\n                if i > 0 and i % self.args.eval_steps == 0:\n                    log.info(f'epoch {epoch+1}:')\n                    self.evaluate()\n                    self.model.train()\n\n                if epoch >= 2 and self.patience >= 3:\n                    self.reload()\n                    \n                if self.patience == -1:\n                    self.reload()\n\n                if self.init_clip_max_norm is not None:\n                    utils.clip_grad_norm_(self.model.parameters(), max_norm=self.init_clip_max_norm)\n\n                avg_loss += loss.item()\n            cnt = len(self.y_train) // self.args.batch_size + 1\n            print(\"Average loss:{:.6f} \".format(avg_loss/cnt))\n            self.evaluate()\n\n\n    def reload(self):\n        log.info(\"Reload the best model...\")\n                    \n        if self.args.n_gpu > 1:\n            self.model.module.load_state_dict(torch.load(self.args.save_path))\n        else:\n            self.model.load_state_dict(torch.load(self.args.save_path))\n        self.adjust_learning_rate()\n        log.info('lr to:' + str(self.args.learning_rate))\n        self.patience = 0\n\n    def adjust_learning_rate(self, decay_rate=.5):\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = param_group['lr'] * decay_rate\n            self.args.learning_rate = param_group['lr']\n        print(\"Decay learning rate to: \", self.args.learning_rate)\n\n\n    def evaluate(self, is_test=False):\n        y_pred = self.predict()\n        with open(self.args.score_file_path, 'w') as output:\n            for score, label in zip(y_pred, self.y_dev):\n                output.write(\n                    str(score) + '\\t' +\n                    str(label) + '\\n'\n                )\n\n        result = self.metrics.evaluate_all_metrics()\n        log.info(f\"Evaluation Result: \\n \" \\\n        f\"MAP:{result[0]}\\tMRR:{result[1]}\\t\" \\\n        f\"P@1:{result[2]}\\tR1:{result[3]}\\t\" \\\n        f\"R2:{result[4]}\\tR5:{result[5]}\"\\\n        f\"ndcg: {result[6]}\")\n\n        if not is_test and result[3] + result[4] + result[5] > self.best_result[3] + self.best_result[4] + self.best_result[5]:\n            log.info(f\"Best Result: \\n \" \\\n            f\"MAP:{self.best_result[0]}\\tMRR:{self.best_result[1]}\\t\" \\\n            f\"P@1:{self.best_result[2]}\\tR1:{self.best_result[3]}\\t\" \\\n            f\"R2:{self.best_result[4]}\\tR5:{self.best_result[5]}\"\\\n            f\"ndcg: {result[6]}\")\n            self.patience = 0\n            self.best_result = result\n            torch.save(get_model_obj(self.model).state_dict(), self.args.save_path)\n            print(\"save model!!!\\n\")\n        else:\n            self.patience += 1\n\n        if not is_test and result[3] + result[4] + result[5] < (self.best_result[3] + self.best_result[4] + self.best_result[5]) * 0.9:\n            self.patience = -1\n\n\n    def predict(self):\n        self.model.eval()\n        y_pred = []\n\n        for i, data in tqdm(enumerate(self.test_set)):\n            with torch.no_grad():\n                batch_u, batch_r = (item.cuda() for item in data)\n                logits1 = self.model(batch_u, batch_r) # , logits2\n                logits = logits1 #  + logits2\n                y_pred += logits.data.cpu().numpy().tolist()\n        return y_pred\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport argparse\nimport pickle\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom dataset import DialogueDataset\nfrom trainer import Trainer\nimport time\n\ntask_dic = {\n    'reddit':'./dataset/reddit/',\n}\nprint(os.listdir(\"../input/reddit-experiment/another_reddit_data))\n\nts = time.strftime('%Y-%m-%d_%H:%M:%S',time.localtime())\n\n## Required parameters\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--task\",\n                    default='reddit',\n                    type=str,\n                    help=\"The dataset used for training and test.\")\nparser.add_argument(\"--is_training\",\n                    default=False,\n                    type=bool,\n                    help=\"Training model or evaluating model?\")\nparser.add_argument(\"--max_utterances\",\n                    default=10,\n                    type=int,\n                    help=\"The maximum number of utterances.\")\nparser.add_argument(\"--emb_len\",\n                    default=300,\n                    type=int)\nparser.add_argument(\"--max_words\",\n                    default=50,\n                    type=int,\n                    help=\"The maximum number of words for each utterance.\")\nparser.add_argument(\"--gru_hidden\",\n                    default=300,\n                    type=int,\n                    help=\"The hidden size of GRU in layer 1\")\nparser.add_argument(\"--learning_rate\",\n                    default=1e-3,\n                    type=float,\n                    help=\"The initial learning rate for Adam.\")\nparser.add_argument(\"--l2_reg\",\n                    default=0.0,\n                    type=float,\n                    help=\"The l2 regularization.\")\nparser.add_argument(\"--epochs\",\n                    default=5,\n                    type=int,\n                    help=\"Total number of training epochs to perform.\")\nparser.add_argument(\"--save_path\",\n                    default=\"./checkpoint/\",\n                    type=str,\n                    help=\"The path to save model.\")\nparser.add_argument(\"--score_file_path\",\n                    default=\"score_file.txt\",\n                    type=str,\n                    help=\"The path to save model.\")\n\nparser.add_argument(\"--eval_steps\",\n                    default=100,\n                    type=int,\n                    help=\"evaluation steps\")\nparser.add_argument(\"--batch_size\",\n                    default=32,\n                    type=int)\nparser.add_argument(\"--local_rank\", \n                    type=int, \n                    default=-1, \n                    help=\"local_rank for distributed training on gpus\")\n\nparser.add_argument(\"--n_gpu\",\n                    default=4,\n                    type=int)\n\nparser.add_argument(\"--n_layer\",\n                    default=3,\n                    type=int)\n\nparser.add_argument(\"--use_cross_matching\",\n                    default=True,\n                    type=bool)\n\nparser.add_argument(\"--n_filters\",\n                    default=128,\n                    type=int)\n\nparser.add_argument(\"--max_hop\",\n                    default=2,\n                    type=int)\n\n\nparser.add_argument(\"--exact_sigma\",\n                    default=0.001,\n                    type=float)\n\nparser.add_argument(\"--sigma\",\n                    default=0.1,\n                    type=float)\n\n\nparser.add_argument(\"--type_file\",\n                    default='/input/reddit-experiment/dw/weibo/test.type',\n                    type=str,\n                    help=\"relevance of answer (to compute ndcg)\")\n\n\nparser.add_argument(\"--model_file_name\",\n                    default='reddit.impchat.pt',\n                    type=str,\n                    help=\"descrip exp details\")\n\nargs = parser.parse_args()\nif args.is_training:\n    args.save_path += args.task + '.' + IMPChat.__name__ + '.'+ts+\".pt\"\nelse:\n    args.save_path += args.model_file_name\nargs.type_file = task_dic[args.task] + 'test.type'\n\nargs.score_file_path = task_dic[args.task] + args.score_file_path + IMPChat.__name__\n\nprint(args)\nprint(\"Task: \", args.task)\n\nclass MyDataParallel(torch.nn.DataParallel):\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n\ndef train_model():\n    path = task_dic[args.task]\n    X_train_utterances, X_train_responses, y_train = pickle.load(file=open(path+f\"train_{args.task}.pkl\", 'rb')) \n    X_dev_utterances, X_dev_responses, y_dev = pickle.load(file=open(path+f\"dev_{args.task}.pkl\", 'rb')) \n    vocab, word_embeddings = pickle.load(file=open(path + \"vocab_and_embeddings.pkl\", 'rb'))\n\n    model = IMPChat(word_embeddings, args=args) \n    train_dataset = DialogueDataset(X_train_utterances, X_train_responses, y_train)\n    train_set = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n\n    test_dataset = DialogueDataset(X_dev_utterances, X_dev_responses)\n    test_set = DataLoader(test_dataset, batch_size=args.batch_size)\n\n    trainer = Trainer(model, train_set, test_set, y_train, y_dev, args)\n    \n    trainer.to_train()\n\n\ndef test_model():\n    path = task_dic[args.task]\n    X_dev_utterances, X_dev_responses, y_dev = pickle.load(file=open(path+f\"test_{args.task}.pkl\", 'rb'))\n    vocab, word_embeddings = pickle.load(file=open(path + \"vocab_and_embeddings.pkl\", 'rb'))\n\n    model = IMPChat(word_embeddings, args=args) \n    model.load_model(args.save_path)\n    test_dataset = DialogueDataset(X_dev_utterances, X_dev_responses)\n    test_set = DataLoader(test_dataset, batch_size=args.batch_size)\n    trainer = Trainer(model, None, test_set, None, y_dev, args)    \n    trainer.evaluate(is_test=True)\n\nif __name__ == '__main__':\n    start = time.time()\n    if args.is_training:\n        train_model()\n        test_model()\n    else:\n        test_model()\n    end = time.time()\n    print(\"use time: \", (end-start)/60, \" min\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T03:24:40.182812Z","iopub.execute_input":"2022-11-25T03:24:40.183660Z","iopub.status.idle":"2022-11-25T03:24:40.296314Z","shell.execute_reply.started":"2022-11-25T03:24:40.183557Z","shell.execute_reply":"2022-11-25T03:24:40.294620Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/334096874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimpchat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMPChat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'impchat'"],"ename":"ModuleNotFoundError","evalue":"No module named 'impchat'","output_type":"error"}]}]}